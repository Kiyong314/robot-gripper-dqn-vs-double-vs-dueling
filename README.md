# Robot Gripper DQN vs Double DQN vs Dueling DQN

IRB360 ë¸íƒ€ ë¡œë´‡ê³¼ ì§„ê³µ ì»µì„ ì‚¬ìš©í•œ Pick-and-Place ì‘ì—…ì„ ìœ„í•œ Deep Reinforcement Learning êµ¬í˜„.  
DQN, Double DQN, Dueling DQN ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

![Grasp Visualization](visualization.grasp.png)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **CoppeliaSim ì‹œë®¬ë ˆì´ì…˜** í™˜ê²½ì—ì„œ IRB360 ë¸íƒ€ ë¡œë´‡ì´ ë¬¼ì²´ë¥¼ ì§‘ì–´ ì˜¬ë¦¬ëŠ” **Grasp** ì‘ì—…ì„ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **3ê°€ì§€ DQN ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**
  - **DQN**: ê¸°ë³¸ Deep Q-Network
  - **Double DQN**: Qê°’ ê³¼ëŒ€ì¶”ì • ë°©ì§€
  - **Dueling DQN**: Value + Advantage ë¶„ë¦¬ êµ¬ì¡°

- **IRB360 ë¸íƒ€ ë¡œë´‡ ìµœì í™”**
  - ì§„ê³µ ì»µ ê·¸ë¦¬í¼ (íšŒì „ ë¶ˆí•„ìš” â†’ í•™ìŠµ ì†ë„ í–¥ìƒ)
  - ZMQ Remote APIë¡œ CoppeliaSim ì—°ë™

- **Curriculum Learning ê¸°ë°˜ íƒìƒ‰ ì „ëµ**
  - ì´ˆê¸°(0~500): ë¬¼ì²´ ìœ„ì¹˜ì—ì„œë§Œ íƒìƒ‰
  - ì¤‘ë°˜(500~1000): 80% ë¬¼ì²´ + 20% ì „ì²´ ì˜ì—­
  - í›„ë°˜(1000+): ì „ì²´ ì˜ì—­ íƒìƒ‰ (ë°”ë‹¥ íšŒí”¼ í•™ìŠµ)

## ğŸ› ï¸ ì„¤ì¹˜ ë°©ë²•

### 1. í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.8+
- CoppeliaSim 4.4+ (ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½)
- CUDA (ì„ íƒ, GPU ê°€ì†ìš©)

### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Kiyong314/robot-gripper-dqn-vs-double-vs-dueling.git
cd robot-gripper-dqn-vs-double-vs-dueling

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ë³„)

```bash
# CPU only
pip install torch torchvision

# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. CoppeliaSim ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘

CoppeliaSimì„ ì‹¤í–‰í•˜ê³  `simulation.ttt` ì”¬ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

### 2. í•™ìŠµ ì‹¤í–‰

```bash
# ê¸°ë³¸ DQN í•™ìŠµ
python main_irb360.py --is_sim --save_visualizations

# Double DQN í•™ìŠµ
python main_irb360.py --is_sim --double_dqn --save_visualizations

# Dueling DQN í•™ìŠµ
python main_irb360.py --is_sim --dueling_dqn --save_visualizations

# Double + Dueling DQN í•™ìŠµ
python main_irb360.py --is_sim --double_dqn --dueling_dqn --save_visualizations
```

### 3. ì£¼ìš” ì¸ì ì„¤ëª…

| ì¸ì | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `--is_sim` | False | ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™” |
| `--double_dqn` | False | Double DQN ì‚¬ìš© |
| `--dueling_dqn` | False | Dueling DQN ì‚¬ìš© |
| `--num_obj` | 10 | ì‹œë®¬ë ˆì´ì…˜ ë¬¼ì²´ ê°œìˆ˜ |
| `--save_visualizations` | False | ì˜ˆì¸¡ ì‹œê°í™” ì €ì¥ |
| `--experience_replay` | True | Experience Replay ì‚¬ìš© |
| `--target_update_freq` | 100 | Target network ì—…ë°ì´íŠ¸ ì£¼ê¸° |
| `--gripper_diameter` | 0.015 | ê·¸ë¦¬í¼ ì§€ë¦„ (m) |

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
robot-gripper-dqn-vs-double-vs-dueling/
â”œâ”€â”€ main_irb360.py          # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ DQNModels.py            # DQN/Dueling DQN ëª¨ë¸ ì •ì˜
â”œâ”€â”€ DQNTrainer.py           # í•™ìŠµ ë¡œì§ (Double DQN í¬í•¨)
â”œâ”€â”€ network.py              # FeatureTrunk (DenseNet ê¸°ë°˜)
â”œâ”€â”€ utils.py                # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ logger.py               # í•™ìŠµ ë¡œê·¸ ê´€ë¦¬
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ simulation.ttt          # CoppeliaSim ì”¬ íŒŒì¼
â”œâ”€â”€ objects/                # 3D ë¬¼ì²´ ëª¨ë¸
â”‚   â””â”€â”€ blocks/             # ë¸”ë¡ ê°ì²´ (.obj)
â”œâ”€â”€ test/                   # í…ŒìŠ¤íŠ¸ ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜
â”‚   â”œâ”€â”€ robot_zmq_irb360.py # ë¡œë´‡ ZMQ í†µì‹  í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ test_camera.py      # ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_calibration.py # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸
â””â”€â”€ logs/                   # í•™ìŠµ ë¡œê·¸ (ìë™ ìƒì„±)
    â””â”€â”€ YYYY-MM-DD.HH.MM.SS/
        â”œâ”€â”€ data/           # ì´ë¯¸ì§€ ë°ì´í„°
        â”œâ”€â”€ models/         # ëª¨ë¸ ìŠ¤ëƒ…ìƒ·
        â”œâ”€â”€ transitions/    # ìƒíƒœ ì „ì´ ë°ì´í„°
        â””â”€â”€ visualizations/ # ì˜ˆì¸¡ ì‹œê°í™”
```

## ğŸ”¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

### DQN (Deep Q-Network)
ê¸°ë³¸ Q-learningì— ì‹ ê²½ë§ì„ ì ìš©í•œ ë°©ë²•.

### Double DQN
- **ë¬¸ì œ**: DQNì€ Qê°’ì„ ê³¼ëŒ€ì¶”ì •í•˜ëŠ” ê²½í–¥
- **í•´ê²°**: í–‰ë™ ì„ íƒê³¼ Qê°’ í‰ê°€ì— ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- **ìˆ˜ì‹**: `Q_target = r + Î³ * Q_target(s', argmax_a Q_main(s', a))`

### Dueling DQN
- **ì•„ì´ë””ì–´**: Qê°’ì„ Value(ìƒíƒœ ê°€ì¹˜)ì™€ Advantage(í–‰ë™ ì´ì )ë¡œ ë¶„ë¦¬
- **ìˆ˜ì‹**: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`
- **ì¥ì **: ìƒíƒœ ìì²´ì˜ ê°€ì¹˜ë¥¼ ë” ì˜ í•™ìŠµ

## ğŸ“Š í•™ìŠµ ê²°ê³¼

í•™ìŠµ ì¤‘ ìƒì„±ë˜ëŠ” ì‹œê°í™”:
- `visualization.grasp.png`: í˜„ì¬ ì˜ˆì¸¡ íˆíŠ¸ë§µ
- `visualization.best_grasp.png`: ì„ íƒëœ ìµœì  ê·¸ë¦¬í•‘ ìœ„ì¹˜

ë¡œê·¸ í´ë”ì—ì„œ í•™ìŠµ ê³¡ì„  í™•ì¸:
- `predicted-value.log`: ì˜ˆì¸¡ Qê°’
- `label-value.log`: ì‹¤ì œ ë ˆì´ë¸” ê°’
- `reward-value.log`: ë³´ìƒ ê°’
- `grasp-success.log`: ê·¸ë¦¬í•‘ ì„±ê³µ ì—¬ë¶€

## ğŸ”§ ìº˜ë¦¬ë¸Œë ˆì´ì…˜

ì¹´ë©”ë¼-ë¡œë´‡ ìº˜ë¦¬ë¸Œë ˆì´ì…˜:
```bash
cd test
python compute_calibration.py
```

ìƒì„±ë˜ëŠ” íŒŒì¼:
- `camera_calibration.npy`: Homography í–‰ë ¬
- `camera_calibration_inv.npy`: ì—­ë³€í™˜ í–‰ë ¬

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- [DQN Paper](https://www.nature.com/articles/nature14236) - Mnih et al., 2015
- [Double DQN Paper](https://arxiv.org/abs/1509.06461) - Van Hasselt et al., 2016
- [Dueling DQN Paper](https://arxiv.org/abs/1511.06581) - Wang et al., 2016
- [Original Pick-to-Place Repository](https://github.com/marwan-AI/Learning-Pick-to-Place-Objects-in-a-cluttered-scene-using-deep-reinforcement-learning)

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ‘¤ Author

- GitHub: [@Kiyong314](https://github.com/Kiyong314)

