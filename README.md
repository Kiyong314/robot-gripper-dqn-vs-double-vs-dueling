# Robot Gripper: DQN vs Double DQN vs Dueling DQN

IRB360 ë¸íƒ€ ë¡œë´‡ê³¼ ì§„ê³µ ê·¸ë¦¬í¼ë¥¼ ì‚¬ìš©í•œ Pick-and-Place ì‘ì—… í•™ìŠµ  
**DQN, Double DQN, Dueling DQN** ì„¸ ê°€ì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ë¶„ì„

![Grasp Visualization](visualization.grasp.png)

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

CoppeliaSim ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ IRB360 ë¸íƒ€ ë¡œë´‡ì´ ì§„ê³µ ê·¸ë¦¬í¼ë¡œ ë¬¼ì²´ë¥¼ ì§‘ëŠ” **Grasp ì‘ì—…**ì„ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **3ê°€ì§€ DQN ì•Œê³ ë¦¬ì¦˜**: Standard DQN, Double DQN, Dueling DQN
- **IRB360 ë¸íƒ€ ë¡œë´‡**: ì§„ê³µ ê·¸ë¦¬í¼ (íšŒì „ ë¶ˆí•„ìš” â†’ í•™ìŠµ ì†ë„ 2ë°° í–¥ìƒ)
- **Curriculum Learning**: ë‹¨ê³„ì  íƒìƒ‰ ì˜ì—­ í™•ì¥ (ë¬¼ì²´ â†’ ì „ì²´ ì˜ì—­)
- **ë°”ë‹¥ ê°ì§€**: ê·¸ë¦¬í¼ ì˜ì—­ ë‚´ Zê°’ ê²€ì¦ìœ¼ë¡œ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬

---

## ğŸ› ï¸ ì„¤ì¹˜

### 1. í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- **Python 3.8+**
- **CoppeliaSim 4.4+** ([ë‹¤ìš´ë¡œë“œ](https://www.coppeliarobotics.com/downloads))
- **CUDA** (ì„ íƒ, GPU ê°€ì†ìš©)

### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Kiyong314/robot-gripper-dqn-vs-double-vs-dueling.git
cd robot-gripper-dqn-vs-double-vs-dueling

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch torchvision numpy opencv-python matplotlib scipy
```

### 3. CoppeliaSim ì”¬ ë¡œë“œ

1. CoppeliaSim ì‹¤í–‰
2. `simulation1.ttt` íŒŒì¼ ì—´ê¸°
3. ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ (â–¶ï¸ ë²„íŠ¼)

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ğŸ“š 1. í•™ìŠµ (Training)

#### Standard DQN
```bash
python main_irb360.py --is_sim --obj_mesh_dir objects/blocks --num_obj 10 --grasp_rewards --experience_replay --save_visualizations
```

#### Double DQN
```bash
python main_irb360.py --is_sim --obj_mesh_dir objects/blocks --num_obj 10 --double_dqn --target_update_freq 30 --grasp_rewards --experience_replay --save_visualizations
```

#### Dueling DQN
```bash
python main_irb360.py --is_sim --obj_mesh_dir objects/blocks --num_obj 10 --dueling_dqn --grasp_rewards --experience_replay --save_visualizations
```

#### Double + Dueling DQN (ê¶Œì¥)
```bash
python main_irb360.py --is_sim --obj_mesh_dir objects/blocks --num_obj 10 --double_dqn --dueling_dqn --target_update_freq 30 --grasp_rewards --experience_replay --save_visualizations
```

**í•™ìŠµ ê²°ê³¼**: `logs/YYYY-MM-DD.HH.MM.SS/` í´ë”ì— ìë™ ì €ì¥

---

### ğŸ§ª 2. í…ŒìŠ¤íŠ¸ (Testing)

í•™ìŠµëœ ëª¨ë¸ë¡œ ì„±ëŠ¥ í‰ê°€:

```bash
python main_irb360.py \
    --is_sim \
    --obj_mesh_dir objects/blocks \
    --num_obj 10 \
    --is_testing \
    --load_snapshot \
    --snapshot_file "saved_model\DQN\snapshot-002100.reinforcement.pth" \
    --max_test_trials 30 \
    --save_visualizations
```

**ì˜µì…˜ ì„¤ëª…**:
- `--max_test_trials 30`: 30íšŒ ì‹œë„ í›„ ìë™ ì¢…ë£Œ
- `--save_visualizations`: Q-value íˆíŠ¸ë§µ ì €ì¥

---

### ğŸ“Š 3. ëª¨ë¸ í‰ê°€ (Evaluation)

ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ í‰ê°€í•˜ê³  ê²°ê³¼ ë¹„êµ:

```bash
# ë‹¨ì¼ ëª¨ë¸ í‰ê°€
python evaluate_checkpoints.py logs/2025-12-07.12.29.59_dqn/models --trials 30 --objects 10

# ê²°ê³¼ ì‹œê°í™”
python plot_evaluation_results.py evaluation/2025-12-07.12.29.59_dqn/evaluation_results.csv
```

**ì¶œë ¥**:
- `evaluation/{model_name}/evaluation_results.csv`: ì„±ê³µë¥ , Q-value í†µê³„
- `evaluation/{model_name}/iter_XXXXXX/`: ê° ì²´í¬í¬ì¸íŠ¸ë³„ ìƒì„¸ ë¡œê·¸

---

### ğŸ” 4. í•™ìŠµ ê³„ì†í•˜ê¸° (Resume Training)

ì¤‘ë‹¨ëœ í•™ìŠµ ì¬ê°œ:

```bash
python main_irb360.py \
    --is_sim \
    --obj_mesh_dir objects/blocks \
    --num_obj 10 \
    --double_dqn --dueling_dqn \
    --load_snapshot \
    --snapshot_file "logs/2025-12-07.12.29.59/models/snapshot-001250.reinforcement.pth" \
    --logging_directory "logs/2025-12-07.12.29.59" \
    --continue_logging \
    --grasp_rewards --experience_replay --save_visualizations
```

---

## âš™ï¸ ì£¼ìš” ì˜µì…˜

| ì˜µì…˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `--is_sim` | False | ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™” (**í•„ìˆ˜**) |
| `--double_dqn` | False | Double DQN ì‚¬ìš© (Qê°’ ê³¼ëŒ€ì¶”ì • ë°©ì§€) |
| `--dueling_dqn` | False | Dueling DQN ì‚¬ìš© (Value/Advantage ë¶„ë¦¬) |
| `--num_obj` | 10 | ì‹œë®¬ë ˆì´ì…˜ ë¬¼ì²´ ê°œìˆ˜ |
| `--target_update_freq` | 100 | Target network ì—…ë°ì´íŠ¸ ì£¼ê¸° (iterations) |
| `--experience_replay` | True | Experience Replay ì‚¬ìš© |
| `--grasp_rewards` | True | ë³´ìƒ ê¸°ë°˜ í•™ìŠµ í™œì„±í™” |
| `--save_visualizations` | False | Q-value íˆíŠ¸ë§µ ì €ì¥ (0.6ì´ˆ/iteration ì¶”ê°€) |
| `--max_test_trials` | 30 | í…ŒìŠ¤íŠ¸ ëª¨ë“œ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ |
| `--gripper_diameter` | 0.005 | ê·¸ë¦¬í¼ ì§€ë¦„ (m), ë°”ë‹¥ ê°ì§€ ì˜ì—­ |

---

## ğŸ“‚ í•™ìŠµ ê²°ê³¼ í™•ì¸

```
logs/YYYY-MM-DD.HH.MM.SS/
â”œâ”€â”€ data/                  # RGB-D ì´ë¯¸ì§€ ë°ì´í„°
â”œâ”€â”€ models/                # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (.pth)
â”‚   â”œâ”€â”€ snapshot-000000.reinforcement.pth
â”‚   â”œâ”€â”€ snapshot-000050.reinforcement.pth
â”‚   â””â”€â”€ ...
â”œâ”€â”€ transitions/           # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ grasp-success.log.txt      # ì„±ê³µ(1.0), ì‹¤íŒ¨(0.0), ë°”ë‹¥(-1.0)
â”‚   â”œâ”€â”€ predicted-value.log.txt    # ì˜ˆì¸¡ Qê°’
â”‚   â”œâ”€â”€ reward-value.log.txt       # ë³´ìƒ ê°’
â”‚   â”œâ”€â”€ epsilon.log.txt            # Epsilon-greedy íƒìƒ‰ ê¸°ë¡
â”‚   â””â”€â”€ object-count.log.txt       # ì”¬ ë‚´ ë¬¼ì²´ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
â””â”€â”€ visualizations/        # Q-value íˆíŠ¸ë§µ ì´ë¯¸ì§€
    â”œâ”€â”€ 000000.grasp.png
    â””â”€â”€ ...
```

---

## ğŸ”¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

### 1. Standard DQN
- ê¸°ë³¸ Deep Q-Network
- Q-learningì— ì‹ ê²½ë§ ì ìš©

### 2. Double DQN
- **ë¬¸ì œ**: Qê°’ ê³¼ëŒ€ì¶”ì • (Overestimation Bias)
- **í•´ê²°**: í–‰ë™ ì„ íƒ(Main)ê³¼ Qê°’ í‰ê°€(Target)ì— ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- **ìˆ˜ì‹**: `Q_target = r + Î³ * Q_target(s', argmax_a Q_main(s', a))`

### 3. Dueling DQN
- **ì•„ì´ë””ì–´**: Qê°’ì„ Value(ìƒíƒœ ê°€ì¹˜)ì™€ Advantage(í–‰ë™ ì´ì )ë¡œ ë¶„ë¦¬
- **ìˆ˜ì‹**: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`
- **ì¥ì **: ëª¨ë“  í–‰ë™ì˜ Qê°’ì„ ëª…ì‹œì ìœ¼ë¡œ í‰ê°€í•˜ì§€ ì•Šì•„ë„ ìƒíƒœ ê°€ì¹˜ í•™ìŠµ

---

## ğŸ”§ ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì„ íƒ)

ë¡œë´‡ ì¢Œí‘œê³„ì™€ ì´ë¯¸ì§€ ì¢Œí‘œê³„ ë§¤í•‘ì„ ìœ„í•œ Homography í–‰ë ¬ ê³„ì‚°:

```bash
cd test
python compute_calibration.py
```

**ìƒì„± íŒŒì¼**:
- `camera_calibration.npy`: Pixel â†’ World ë³€í™˜ í–‰ë ¬
- `camera_calibration_inv.npy`: World â†’ Pixel ë³€í™˜ í–‰ë ¬

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
robot-gripper-dqn-vs-double-vs-dueling/
â”œâ”€â”€ main_irb360.py              # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ DQNModels.py                # DQN/Dueling DQN ëª¨ë¸
â”œâ”€â”€ DQNTrainer.py               # í•™ìŠµ ë¡œì§ (Double DQN í¬í•¨)
â”œâ”€â”€ network.py                  # FeatureTrunk (DenseNet-121)
â”œâ”€â”€ utils.py                    # Heightmap ìƒì„±, ì¢Œí‘œ ë³€í™˜
â”œâ”€â”€ logger.py                   # ë¡œê·¸ ê´€ë¦¬
â”œâ”€â”€ evaluate_checkpoints.py     # ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ plot_evaluation_results.py  # ê²°ê³¼ ì‹œê°í™”
â”œâ”€â”€ simulation.ttt              # CoppeliaSim ì”¬ íŒŒì¼
â”œâ”€â”€ objects/blocks/             # 3D ë¬¼ì²´ ëª¨ë¸ (.obj)
â””â”€â”€ test/
    â”œâ”€â”€ robot_zmq_irb360.py     # IRB360 ZMQ í†µì‹  í´ë˜ìŠ¤
    â”œâ”€â”€ test_camera.py          # ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸
    â””â”€â”€ compute_calibration.py  # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚°
```

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

### ë…¼ë¬¸
- **DQN**: Mnih et al., "Human-level control through deep reinforcement learning", *Nature*, 2015
- **Double DQN**: Van Hasselt et al., "Deep Reinforcement Learning with Double Q-learning", *AAAI*, 2016
- **Dueling DQN**: Wang et al., "Dueling Network Architectures for Deep Reinforcement Learning", *ICML*, 2016

### ì°¸ê³  ì½”ë“œ
ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤:

1. **Marwan Qaid Mohammed** - [Learning-Pick-to-Place-Objects](https://github.com/Marwanon/Learning-Pick-to-Place-Objects-in-a-cluttered-scene-using-deep-reinforcement-learning)
   - ë…¼ë¬¸: "Learning Pick to Place Objects using Self-supervised Learning with Minimal Training Resources", *IJACSA*, 2021
   - ì‚¬ìš©: `network.py`, `logger.py`, `DQNModels.py` (ì¼ë¶€), `DQNTrainer.py` (ì¼ë¶€)

2. **Andy Zeng et al.** - [Visual-Pushing-Grasping](https://github.com/andyzeng/visual-pushing-grasping)

---

## âœ¨ ë³¸ í”„ë¡œì íŠ¸ì˜ ë…ìì  ê¸°ì—¬

| ê¸°ëŠ¥ | íŒŒì¼ | ì„¤ëª… |
|------|------|------|
| **Double DQN** | `DQNTrainer.py` | Target Network ë¶„ë¦¬, Qê°’ ê³¼ëŒ€ì¶”ì • ë°©ì§€ |
| **Dueling DQN** | `DQNModels.py` | Value + Advantage ìŠ¤íŠ¸ë¦¼ ë¶„ë¦¬ |
| **IRB360 í†µí•©** | `robot_zmq_irb360.py` | CoppeliaSim ZMQ API ê¸°ë°˜ ë¡œë´‡ ì œì–´ |
| **ì§„ê³µ ê·¸ë¦¬í¼** | `robot_zmq_irb360.py` | í¡ì°©/í•´ì œ ì‹œê·¸ë„, ì„¼ì„œ ê¸°ë°˜ í•˜ê°• |
| **Curriculum Learning** | `main_irb360.py` | ë‹¨ê³„ì  íƒìƒ‰ ì˜ì—­ í™•ì¥ ì „ëµ |
| **ë°”ë‹¥ ê°ì§€** | `main_irb360.py` | ê·¸ë¦¬í¼ ì˜ì—­ Zê°’ ê²€ì¦ ë° ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬ |
| **ëª¨ë¸ í‰ê°€** | `evaluate_checkpoints.py` | ìë™í™”ëœ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ ë¶„ì„ |

---

## ğŸ“ License

MIT License

## ğŸ‘¤ Author

GitHub: [@Kiyong314](https://github.com/Kiyong314)

---

## ğŸ’¡ Tips

### í•™ìŠµ ì†ë„ í–¥ìƒ
- `--save_visualizations` ì œê±° ì‹œ **40% ì†ë„ í–¥ìƒ**
- GPU ì‚¬ìš© ì‹œ **10ë°° ì†ë„ í–¥ìƒ** (CUDA í•„ìš”)

### ì„±ê³µë¥  ê°œì„ 
- `--target_update_freq 30`: Double DQN ì—…ë°ì´íŠ¸ ì£¼ê¸° ë‹¨ì¶•
- `--num_obj 5`: ë¬¼ì²´ ê°œìˆ˜ë¥¼ ì¤„ì—¬ ì´ˆê¸° í•™ìŠµ ë‚œì´ë„ ë‚®ì¶¤

### í•™ìŠµ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ ì„±ê³µë¥  í™•ì¸
tail -f logs/YYYY-MM-DD.HH.MM.SS/transitions/grasp-success.log.txt

# Epsilon íƒìƒ‰ ë¶„ì„
python analyze_epsilon.py logs/YYYY-MM-DD.HH.MM.SS
```
