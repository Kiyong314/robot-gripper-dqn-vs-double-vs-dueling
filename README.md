# Robot Gripper DQN vs Double DQN vs Dueling DQN

IRB360 ë¸íƒ€ ë¡œë´‡ê³¼ ì§„ê³µ ì»µì„ ì‚¬ìš©í•œ Pick-and-Place ì‘ì—…ì„ ìœ„í•œ Deep Reinforcement Learning êµ¬í˜„.  
DQN, Double DQN, Dueling DQN ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

![Grasp Visualization](visualization.grasp.png)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **CoppeliaSim ì‹œë®¬ë ˆì´ì…˜** í™˜ê²½ì—ì„œ IRB360 ë¸íƒ€ ë¡œë´‡ì´ ë¬¼ì²´ë¥¼ ì§‘ì–´ ì˜¬ë¦¬ëŠ” **Grasp** ì‘ì—…ì„ ê°•í™”í•™ìŠµìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **3ê°€ì§€ DQN ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**
  - **DQN**: ê¸°ë³¸ Deep Q-Network
  - **Double DQN**: Qê°’ ê³¼ëŒ€ì¶”ì • ë°©ì§€
  - **Dueling DQN**: Value + Advantage ë¶„ë¦¬ êµ¬ì¡°

- **IRB360 ë¸íƒ€ ë¡œë´‡ ìµœì í™”**
  - ì§„ê³µ ì»µ ê·¸ë¦¬í¼ (íšŒì „ ë¶ˆí•„ìš” â†’ í•™ìŠµ ì†ë„ í–¥ìƒ)
  - ZMQ Remote APIë¡œ CoppeliaSim ì—°ë™

- **Curriculum Learning ê¸°ë°˜ íƒìƒ‰ ì „ëµ**
  - ì´ˆê¸°(0~500): ë¬¼ì²´ ìœ„ì¹˜ì—ì„œë§Œ íƒìƒ‰
  - ì¤‘ë°˜(500~1000): 80% ë¬¼ì²´ + 20% ì „ì²´ ì˜ì—­
  - í›„ë°˜(1000+): ì „ì²´ ì˜ì—­ íƒìƒ‰ (ë°”ë‹¥ íšŒí”¼ í•™ìŠµ)

## ğŸ› ï¸ ì„¤ì¹˜ ë°©ë²•

### 1. í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.8+
- CoppeliaSim 4.4+ (ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½)
- CUDA (ì„ íƒ, GPU ê°€ì†ìš©)

### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Kiyong314/robot-gripper-dqn-vs-double-vs-dueling.git
cd robot-gripper-dqn-vs-double-vs-dueling

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ë³„)

```bash
# CPU only
pip install torch torchvision

# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. CoppeliaSim ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘

CoppeliaSimì„ ì‹¤í–‰í•˜ê³  `simulation.ttt` ì”¬ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

### 2. í•™ìŠµ ì‹¤í–‰

```bash
# ê¸°ë³¸ DQN í•™ìŠµ
python main_irb360.py --is_sim --save_visualizations

# Double DQN í•™ìŠµ
python main_irb360.py --is_sim --double_dqn --save_visualizations

# Dueling DQN í•™ìŠµ
python main_irb360.py --is_sim --dueling_dqn --save_visualizations

# Double + Dueling DQN í•™ìŠµ
python main_irb360.py --is_sim --double_dqn --dueling_dqn --save_visualizations
```

### 3. ì£¼ìš” ì¸ì ì„¤ëª…

| ì¸ì | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `--is_sim` | False | ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ í™œì„±í™” |
| `--double_dqn` | False | Double DQN ì‚¬ìš© |
| `--dueling_dqn` | False | Dueling DQN ì‚¬ìš© |
| `--num_obj` | 10 | ì‹œë®¬ë ˆì´ì…˜ ë¬¼ì²´ ê°œìˆ˜ |
| `--save_visualizations` | False | ì˜ˆì¸¡ ì‹œê°í™” ì €ì¥ |
| `--experience_replay` | True | Experience Replay ì‚¬ìš© |
| `--target_update_freq` | 100 | Target network ì—…ë°ì´íŠ¸ ì£¼ê¸° |
| `--gripper_diameter` | 0.015 | ê·¸ë¦¬í¼ ì§€ë¦„ (m) |

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
robot-gripper-dqn-vs-double-vs-dueling/
â”œâ”€â”€ main_irb360.py          # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ DQNModels.py            # DQN/Dueling DQN ëª¨ë¸ ì •ì˜
â”œâ”€â”€ DQNTrainer.py           # í•™ìŠµ ë¡œì§ (Double DQN í¬í•¨)
â”œâ”€â”€ network.py              # FeatureTrunk (DenseNet ê¸°ë°˜)
â”œâ”€â”€ utils.py                # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ logger.py               # í•™ìŠµ ë¡œê·¸ ê´€ë¦¬
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ simulation.ttt          # CoppeliaSim ì”¬ íŒŒì¼
â”œâ”€â”€ objects/                # 3D ë¬¼ì²´ ëª¨ë¸
â”‚   â””â”€â”€ blocks/             # ë¸”ë¡ ê°ì²´ (.obj)
â”œâ”€â”€ test/                   # í…ŒìŠ¤íŠ¸ ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜
â”‚   â”œâ”€â”€ robot_zmq_irb360.py # ë¡œë´‡ ZMQ í†µì‹  í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ test_camera.py      # ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_calibration.py # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸
â””â”€â”€ logs/                   # í•™ìŠµ ë¡œê·¸ (ìë™ ìƒì„±)
    â””â”€â”€ YYYY-MM-DD.HH.MM.SS/
        â”œâ”€â”€ data/           # ì´ë¯¸ì§€ ë°ì´í„°
        â”œâ”€â”€ models/         # ëª¨ë¸ ìŠ¤ëƒ…ìƒ·
        â”œâ”€â”€ transitions/    # ìƒíƒœ ì „ì´ ë°ì´í„°
        â””â”€â”€ visualizations/ # ì˜ˆì¸¡ ì‹œê°í™”
```

## ğŸ”¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

### DQN (Deep Q-Network)
ê¸°ë³¸ Q-learningì— ì‹ ê²½ë§ì„ ì ìš©í•œ ë°©ë²•.

### Double DQN
- **ë¬¸ì œ**: DQNì€ Qê°’ì„ ê³¼ëŒ€ì¶”ì •í•˜ëŠ” ê²½í–¥
- **í•´ê²°**: í–‰ë™ ì„ íƒê³¼ Qê°’ í‰ê°€ì— ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
- **ìˆ˜ì‹**: `Q_target = r + Î³ * Q_target(s', argmax_a Q_main(s', a))`

### Dueling DQN
- **ì•„ì´ë””ì–´**: Qê°’ì„ Value(ìƒíƒœ ê°€ì¹˜)ì™€ Advantage(í–‰ë™ ì´ì )ë¡œ ë¶„ë¦¬
- **ìˆ˜ì‹**: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`
- **ì¥ì **: ìƒíƒœ ìì²´ì˜ ê°€ì¹˜ë¥¼ ë” ì˜ í•™ìŠµ

## ğŸ“Š í•™ìŠµ ê²°ê³¼

í•™ìŠµ ì¤‘ ìƒì„±ë˜ëŠ” ì‹œê°í™”:
- `visualization.grasp.png`: í˜„ì¬ ì˜ˆì¸¡ íˆíŠ¸ë§µ
- `visualization.best_grasp.png`: ì„ íƒëœ ìµœì  ê·¸ë¦¬í•‘ ìœ„ì¹˜

ë¡œê·¸ í´ë”ì—ì„œ í•™ìŠµ ê³¡ì„  í™•ì¸:
- `predicted-value.log`: ì˜ˆì¸¡ Qê°’
- `label-value.log`: ì‹¤ì œ ë ˆì´ë¸” ê°’
- `reward-value.log`: ë³´ìƒ ê°’
- `grasp-success.log`: ê·¸ë¦¬í•‘ ì„±ê³µ ì—¬ë¶€

## ğŸ”§ ìº˜ë¦¬ë¸Œë ˆì´ì…˜

ì¹´ë©”ë¼-ë¡œë´‡ ìº˜ë¦¬ë¸Œë ˆì´ì…˜:
```bash
cd test
python compute_calibration.py
```

ìƒì„±ë˜ëŠ” íŒŒì¼:
- `camera_calibration.npy`: Homography í–‰ë ¬
- `camera_calibration_inv.npy`: ì—­ë³€í™˜ í–‰ë ¬

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

### ë…¼ë¬¸
- [DQN Paper](https://www.nature.com/articles/nature14236) - Mnih et al., "Human-level control through deep reinforcement learning", Nature, 2015
- [Double DQN Paper](https://arxiv.org/abs/1509.06461) - Van Hasselt et al., "Deep Reinforcement Learning with Double Q-learning", AAAI, 2016
- [Dueling DQN Paper](https://arxiv.org/abs/1511.06581) - Wang et al., "Dueling Network Architectures for Deep Reinforcement Learning", ICML, 2016

### ì°¸ê³  ì½”ë“œ
- [Visual Pushing Grasping](https://github.com/andyzeng/visual-pushing-grasping) - Andy Zeng et al.
- [Learning Pick-to-Place Objects](https://github.com/Marwanon/Learning-Pick-to-Place-Objects-in-a-cluttered-scene-using-deep-reinforcement-learning) - Marwan Qaid Mohammed

---

## ğŸ™ Acknowledgments & Code Attribution

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

### 1. ê¸°ë³¸ DQN êµ¬í˜„
- **Repository**: [Marwanon/Learning-Pick-to-Place-Objects](https://github.com/Marwanon/Learning-Pick-to-Place-Objects-in-a-cluttered-scene-using-deep-reinforcement-learning)
- **Author**: Marwan Qaid Mohammed
- **Citation**: 
  ```
  Marwan Qaid Mohammed, Lee Chung Kwek and Shing Chyi Chua, 
  "Learning Pick to Place Objects using Self-supervised Learning with Minimal Training Resources"
  International Journal of Advanced Computer Science and Applications (IJACSA), 12(10), 2021.
  ```
- **ì‚¬ìš©ëœ íŒŒì¼**: `network.py`, `logger.py`, `DQNModels.py` (ì¼ë¶€), `DQNTrainer.py` (ì¼ë¶€), `utils.py` (ì¼ë¶€)

### 2. Visual Pushing Grasping
- **Repository**: [andyzeng/visual-pushing-grasping](https://github.com/andyzeng/visual-pushing-grasping)
- **Author**: Andy Zeng et al.

---

## âœ¨ ë³¸ í”„ë¡œì íŠ¸ì˜ ë…ìì  ê¸°ì—¬ (My Original Contributions)

| ê¸°ëŠ¥ | íŒŒì¼ | ì„¤ëª… |
|------|------|------|
| **Double DQN** | `DQNTrainer.py` | Target Network ë¶„ë¦¬, Qê°’ ê³¼ëŒ€ì¶”ì • ë°©ì§€ |
| **Dueling DQN** | `DQNModels.py` | Value + Advantage ìŠ¤íŠ¸ë¦¼ ë¶„ë¦¬ ì•„í‚¤í…ì²˜ |
| **IRB360 ë¸íƒ€ ë¡œë´‡ í†µí•©** | `robot_zmq_irb360.py` | CoppeliaSim ZMQ API ê¸°ë°˜ ë¡œë´‡ ì œì–´ (ì™„ì „ ìƒˆë¡œ ì‘ì„±) |
| **ì§„ê³µ ì»µ ì œì–´** | `robot_zmq_irb360.py` | í¡ì°©/í•´ì œ ì‹œê·¸ë„, ì„¼ì„œ ê¸°ë°˜ í•˜ê°• |
| **Curriculum Learning** | `main_irb360.py` | ë‹¨ê³„ì  íƒìƒ‰ ì˜ì—­ í™•ì¥ ì „ëµ |
| **ë°”ë‹¥ ê°ì§€** | `main_irb360.py` | ê·¸ë¦¬í¼ ì˜ì—­ ë‚´ ë°”ë‹¥ ì„ íƒ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨ ë° ìŒìˆ˜ ë³´ìƒ |
| **ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜** | `utils.py`, `test/` | Homography ê¸°ë°˜ ì¢Œí‘œ ë³€í™˜ |
| **Heightmap ê²€ì¦** | `utils.py` | ë¬¼ì²´ ìœ„ì¹˜ì™€ heightmap ì •í™•ë„ ê²€ì¦ |
| **Orthographic ì¹´ë©”ë¼** | `utils.py` | Ortho ì¹´ë©”ë¼ìš© heightmap ìƒì„± |
| **ë™ì¼ ì´ë¯¸ì§€ ê°ì§€** | `main_irb360.py` | ë¬¼ì²´ê°€ ë¡œë´‡ì— ë¶™ì–´ìˆëŠ” ê²½ìš° ê°ì§€ |

---

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ‘¤ Author

- GitHub: [@Kiyong314](https://github.com/Kiyong314)

