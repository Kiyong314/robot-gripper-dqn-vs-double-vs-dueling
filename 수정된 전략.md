# Curriculum Learning 기반 탐색 전략

## 개요

강화학습에서 Epsilon-greedy 탐색 시, 학습 단계에 따라 탐색 영역을 점진적으로 확장하는 Curriculum Learning 방식을 적용했습니다.

## 문제점 (기존 방식)

기존에는 탐색 시 **물체가 있는 위치에서만** 랜덤 선택을 했습니다:

```python
valid_indices = np.argwhere(valid_depth_heightmap > 0.01)  # 물체만
```

**문제:**
- 네트워크가 "바닥 선택 = 실패"를 학습하지 못함
- 실제 inference 시 바닥을 선택할 가능성 존재

## 해결책: Curriculum Learning

학습 단계에 따라 탐색 영역을 점진적으로 확장합니다.

### 학습 단계별 전략

| 단계 | Iteration | 탐색 영역 | 설명 |
|------|-----------|-----------|------|
| 초기 | 0 ~ 500 | 물체 위치만 | 빠른 기본 학습 (효율성 우선) |
| 중반 | 500 ~ 1000 | 80% 물체 + 20% 전체 | 바닥 회피 학습 시작 |
| 후반 | 1000+ | 전체 영역 | 바닥 = 낮은 Q값 완전 학습 |

### 구현 코드 (main_irb360.py)

```python
# 탐색(Exploration): Curriculum Learning 방식
iteration = trainer.iteration

if iteration < 500:
    # 초기: 물체가 있는 위치에서만 탐색 (빠른 기본 학습)
    valid_indices = np.argwhere(valid_depth_heightmap > 0.01)
    exploration_mode = "object_only"
elif iteration < 1000:
    # 중반: 80% 물체 위치 + 20% 전체 영역 (바닥 회피 학습 시작)
    if np.random.random() < 0.8:
        valid_indices = np.argwhere(valid_depth_heightmap > 0.01)
        exploration_mode = "object_only"
    else:
        valid_indices = np.argwhere(valid_depth_heightmap >= 0)
        exploration_mode = "full_area"
else:
    # 후반: 전체 영역에서 탐색 (바닥 = 낮은 Q값 학습)
    valid_indices = np.argwhere(valid_depth_heightmap >= 0)
    exploration_mode = "full_area"
```

## 기대 효과

1. **초기 학습 효율성 유지**: 물체 위치에서만 탐색하여 빠르게 기본 grasp 능력 학습
2. **점진적 난이도 증가**: 중반부터 바닥 선택 경험을 통해 실패 케이스 학습
3. **Robust한 모델**: 후반에는 전체 영역에서 탐색하여 "바닥 = 낮은 Q값" 완전 학습
4. **실제 환경 대응력 향상**: 바닥을 피하는 능력이 네트워크에 내재화됨

## 로그 출력

학습 중 다음과 같은 로그로 현재 탐색 모드를 확인할 수 있습니다:

```
[EXPLORATION] Curriculum mode: object_only (iter=123)
[EXPLORATION] Curriculum mode: full_area (iter=789)
[EXPLORATION] epsilon=0.450, random_action=True
```




python main_irb360.py --is_sim --experience_replay --double_dqn --dueling_dqn --save_visualizations --grasp_rewards --target_update_freq 100
